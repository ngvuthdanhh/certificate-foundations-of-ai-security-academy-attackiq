# ðŸ“– Glossary â€“ Foundations of AI Security

## A
- **Adversarial Attack**: A technique to manipulate AI/ML models by feeding malicious or crafted inputs to cause incorrect outputs.  
- **AI Security**: The discipline of protecting AI systems from threats, risks, and vulnerabilities.  

## D
- **Data Poisoning**: Malicious modification of training data to compromise model performance.  
- **Defense-in-Depth**: A layered security strategy applied to AI systems.  

## E
- **Evasion Attack**: Adversarial input crafted to bypass AI detection or classification.  

## M
- **Model Stealing**: Extracting or replicating a trained ML modelâ€™s behavior without authorization.  

## P
- **Purple Teaming**: A collaborative approach combining Red Team (offensive) and Blue Team (defensive) tactics in AI/ML security.  

## T
- **Threat Model**: A structured representation of potential adversaries, attack vectors, and assets in AI systems.  
