# ðŸ§ª Lab â€“ Evasion Attack

## Objective
Explore adversarial examples that trick AI systems at inference time.

## Steps
1. Train a simple classifier (e.g., CIFAR-10 images).
2. Use an adversarial toolkit (e.g., FGSM method).
3. Generate perturbed samples that look human-identical.
4. Evaluate model predictions on adversarial vs. clean data.

## Expected Outcome
- Model misclassifies adversarial inputs.
- Visual perturbations are minimal to the human eye.

## Defense Strategies
- Adversarial training.
- Input preprocessing (denoising, feature squeezing).
- Ensemble models.
