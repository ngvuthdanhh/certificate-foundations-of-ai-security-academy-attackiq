# ðŸ§ª Module 5 â€“ AI Security Testing

## Red Team
- Simulate adversarial attacks.
- Focus: evasion, poisoning, model extraction.

## Blue Team
- Defend and monitor AI pipelines.
- Develop countermeasures & anomaly detection.

## Purple Team
- Collaborative exercises.
- Leverage **MITRE ATLAS** for shared understanding.
