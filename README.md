# ğŸ›¡ï¸ Foundations of AI Security â€“ AttackIQ Security Academy  

![Course](https://img.shields.io/badge/AttackIQ-Foundations%20of%20AI%20Security-darkred?style=flat-square&logo=openai)
![Status](https://img.shields.io/badge/Status-Completed-brightgreen?style=flat-square&logo=verizon)
![Type](https://img.shields.io/badge/Type-Learning%20Project-orange?style=flat-square&logo=notion)
![Focus](https://img.shields.io/badge/Focus-AI%20Security-informational?style=flat-square&logo=defender)
![Maintainer](https://img.shields.io/badge/Maintainer-ThÃ nh%20Danh-blueviolet?style=flat-square&logo=github)

This repository contains **notes, labs, reports, resources, and certificate of completion** for the *Foundations of AI Security* course by **AttackIQ Security Academy**.  
The course provides a structured foundation in **AI threats, adversarial attacks, security testing, and defense strategies**.  

---

## ğŸ“š Notes  

- ğŸ“„ [`01-introduction.md`](./notes/01-introduction.md) â€“ Introduction to AI Security  
- ğŸ“„ [`02-attack-surface.md`](./notes/02-attack-surface.md) â€“ Understanding AI attack surface  
- ğŸ“„ [`03-adversarial-threats.md`](./notes/03-adversarial-threats.md) â€“ Adversarial threats & attack types  
- ğŸ“„ [`04-defensive-strategies.md`](./notes/04-defensive-strategies.md) â€“ Defensive methodologies  
- ğŸ“„ [`05-ai-security-testing.md`](./notes/05-ai-security-testing.md) â€“ AI security testing approaches  
- ğŸ“„ [`06-incident-response.md`](./notes/06-incident-response.md) â€“ Incident response in AI systems  
- ğŸ“„ [`07-future-directions.md`](./notes/07-future-directions.md) â€“ Future of AI security  
- ğŸ“„ [`08-summary-reflections.md`](./notes/08-summary-reflections.md) â€“ Final reflections  

---

## ğŸ§ª Labs  

- ğŸ­ [`evasion-lab.md`](./labs/evasion-lab.md) â€“ Exploring evasion attacks  
- ğŸ“¥ [`model-extraction-lab.md`](./labs/model-extraction-lab.md) â€“ Model extraction techniques  
- â˜£ï¸ [`poisoning-lab.md`](./labs/poisoning-lab.md) â€“ Data poisoning simulation  
- ğŸ’¬ [`prompt-injection-lab.md`](./labs/prompt-injection-lab.md) â€“ Prompt injection in LLMs  

---

## ğŸ“Š Reports  

- ğŸ›¡ï¸ [`defense_strategies.md`](./report/defense_strategies.md) â€“ AI defense strategies overview  
- ğŸ“‘ [`findings_summary.md`](./report/findings_summary.md) â€“ Summary of findings  
- âš ï¸ [`threat_assessment.md`](./report/threat_assessment.md) â€“ Threat analysis & risk assessment  

---

## ğŸ“‚ Extras  

- ğŸ“‘ [`case-studies.md`](./extras/case-studies.md) â€“ Case studies on AI threats  
- ğŸ“š [`resources.md`](./extras/resources.md) â€“ References & resources  
- â³ [`timeline.md`](./extras/timeline.md) â€“ Timeline of AI security evolution  

---

## ğŸ“– Docs  

- ğŸ“˜ [`glossary.md`](./docs/glossary.md) â€“ AI Security glossary  
- ğŸ“˜ [`index.md`](./docs/index.md) â€“ Index of course content  
- ğŸ“˜ [`references.md`](./docs/references.md) â€“ References & sources  
- ğŸ“˜ [`roadmap.md`](./docs/roadmap.md) â€“ Learning roadmap  
- ğŸ“˜ [`syllabus.md`](./docs/syllabus.md) â€“ Course syllabus  

---

## ğŸ“¸ Screenshots  

| Step                  | Screenshot |
|-----------------------|------------|
| ğŸ« Course Details     | ![](./screenshots/course-details.png) |
| ğŸ“‹ Course Information | ![](./screenshots/course-information.png) |

---

## ğŸ“œ Certificate  

ğŸ“ [`Foundations of AI Security`](./cert/Foundations%20of%20AI%20Security.png)  

---

## ğŸ“ Personal Review  

This course deepened my understanding of **AI-specific security challenges**.  
I learned how adversaries can exploit **attack surfaces in ML models** using techniques like **evasion, extraction, poisoning, and prompt injection**.  
The **labs provided practical insights** into how attacks unfold and how defenders can mitigate risks.  

What stood out most was the **focus on incident response and defensive strategies** tailored for AI systems, preparing me for real-world applications in **Red Teaming & AI risk analysis**.  

Overall, the course is highly valuable for anyone looking to **build expertise in the emerging field of AI Security**.  

---

## âœï¸ Author  

**ThÃ nh Danh** â€“ Red Team Learner & Security Researcher  

- GitHub: [@ngvuthdanhh](https://github.com/ngvuthdanhh)  
- Email: ngvu.thdanh@gmail.com  

---

## ğŸ“„ License  

This project is licensed under the terms of the **MIT License**.  
See [`LICENSE`](./LICENSE) for full details.  

Â© 2025 ngvuthdanhh. All rights reserved.  
